# Course Project Writeup
##  Practical Machine Learning, September 2015 
### John Q. Murray 

# Overview

The course project was to create a machine learning algorithm using the dataset from Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. 
Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. Velloso et. al. tackled an interesting problem. "In activity recognition using on-body sensing, a large body of work has investigated automatic techniques to discriminate which activity was performed.
So far, only little work has focused on the problem of quantifying how (well) an activity was performed. We refer to the latter as 'qualitative activity recognition.'" As described in their paper, they learned that they could effectively model correct and incorrect activity using 17 data points from on-body sensors. 

The Course Project involved predicting the "classe" values of a predefined Test set of 20 samples, after building a model using a Training set of 19622 samples from this Velloso data. The "classe" variable represented a categorical value indicating the person's qualititative performance 
of the Unilateral Dumbbell Biceps Curl: "exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

# Building the Model

The caret package in R is unbelievably powerful and contains advanced modeling methods. The approach was to explore the data and then try several of the modelling functions,
with minimal processing of the data, to see which approach might be the most promising, and then fine-tune the most promising modelling approach. 

The Velloso research paper suggested the possibility of identifying the 17 most important variables among the datapoints provided in the Train and Test datasets. It was not possible to positively identify these variables based on the documentation, but examining the data demonstrated that much of the Test data was Not Available (NA). 
To reduce the size of the dataset and improve performance of the caret modeling functions, the training and the testing datasets were processed identically, pared down to only those variables in the test dataset that contained valid data. Although still larger than 17, this significantly reduced the number of data points from 160 to 50.

```{r}
library(caret);
inTrain <- read.csv("pml-training.csv", na.strings="NA");
inTest <- read.csv("pml-testing.csv", na.strings="NA");
View(inTest)
# data in most columns in the Test dataset appear as blank or NA; get rid of index to start
inTrain <- inTrain[,-1]
inTest <- inTest[,-1]
# preserve only those data points that have actual data in the Test set
inTrainMinCols <- inTrain[,c(7:10, 36:48, 59:67, 83:85, 101, 115:123, 139, 150:159)]
# perform same processing on Training set and Testing set
inTestMinCols <- inTest[,c(7:10, 36:48, 59:67, 83:85, 101, 115:123, 139, 150:159)]
colnames(inTrainMinCols)

#colnames(inTestMinCols)
# same except problem_id in place of classe
```

A standard CART approach was first tried using the RPART method. The classe value was compared against all other remaining variables. 
However, this approach produced accuracy of only about .50, equivalent to a coin flip:

```{r}
set.seed(1235)
myrpartfit <- train(classe ~ ., method="rpart", data=inTrainMinCols) 
myrpartfit$finalModel
plot(myrpartfit$finalModel, uniform=TRUE, main="Classification Tree")
text(myrpartfit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```

The resulting Classification Tree did not even appear to select the D option! On reflection, this may have been due to use of the entire training set; perhaps the A values 
are bunched in the beginning of the training data. Another attempt could use createDataPartition to split the training data itself on the classe variable.

# Using Cross Validation (CV)

The Random Forests method (RF) automatically incorporates cross-validation into the model. As Leo Breiman and Adele Cutler write (www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm): 
"In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:
Each tree is constructed using a different bootstrap sample from the original data." Because the RF method performs its own CV, the entire training dataset was used during the processing.

```{r}
set.seed(33833)
#myRfFitMinCols <- train(classe~.,data=inTrainMinCols, method="rf", prox=TRUE);
# for purposes of knit, just reload the existing 
myRfFitMinCols <- readRDS("myRFFitMinCols.rds")
``` 

# Calculating the Expected Out of Sample Error

The OOB value estimates the out of sample error rate. In this experiment, the initial run was surprisingly successful, with a rate less than 1 percent:

```{r}
# myRfFitMinCols$finalModel
# Call:
#  randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) 
#               Type of random forest: classification
#                     Number of trees: 500
# No. of variables tried at each split: 25

#        OOB estimate of  error rate: 0.43%
# Confusion matrix:
#      A    B    C    D    E  class.error
# A 5577    2    0    0    1 0.0005376344
# B   20 3772    5    0    0 0.0065841454
# C    0    9 3402   11    0 0.0058445354
# D    0    0   24 3190    2 0.0080845771
# E    0    0    4    6 3597 0.0027723870
```

If there had been issues with the data, the Random Forest would have been re-run using only the most important variables.
Breiman writes: "Another useful option is to do an automatic rerun using only those variables that were most important in the original run."
The varImp method lists the most important variables:

```{r}
varImp(myRfFitMinCols)
```

# Predicting 20 Different Test Cases

The initial model fit using the Random Forest method was used to predict values in the Test dataset, as follows:

```{r}
myPredict <- predict(myRfFitMinCols$finalModel, newdata=inTestMinCols)
myPredict
```

All 20 values were accepted as correct during the initial submission, so no further processing was performed. 
If greater accuracy had been required, the original training set could be partitioned for cross-validation, with training performed on part of the training set and validation conducted on the part left out. The training set could also be pared to the most important variables, as suggested by Breiman. However, because the course project allowed multiple submissions without penalty, using the largest possible training set with the built-in CV enabled a solution in a very rapid manner. 

# Conclusion

Use caret! 
